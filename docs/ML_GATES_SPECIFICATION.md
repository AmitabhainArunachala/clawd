# ML Gates Specification

## Extension to 17-Gate Protocol for ML-Enabled Coding Agents

**Version:** 1.0  
**Date:** 2026-02-05  
**Status:** Draft Specification

---

## Executive Summary

This document specifies five additional gates designed to future-proof the 17-gate coding protocol for ML-enabled coding agents. These gates address critical gaps identified in model transparency, training data provenance, ML security, and data lineage tracking.

### Research Foundation

This specification is based on comprehensive analysis of:

1. **Google Model Cards** (Mitchell et al., 2019) - Framework for transparent model reporting
2. **Hugging Face Model Card Standards** - Industry-standard metadata and documentation practices
3. **OWASP ML Security Top 10** - Comprehensive security threat taxonomy for ML systems
4. **MITRE ATLAS** - Adversarial ML threat matrix
5. **OpenLineage** - Open standard for data lineage collection and analysis

---

## Gate 18: Model Card Verification Gate

### Purpose

Ensure all ML models used or generated by the coding agent are accompanied by comprehensive, verified model cards that document intended use, limitations, biases, and performance characteristics.

### Rationale

Per Google's Model Cards framework, "released models should be accompanied by documentation detailing their performance characteristics." Without model cards, agents cannot make informed decisions about model suitability for specific tasks.

### Gate Criteria

| Criterion | Priority | Description |
|-----------|----------|-------------|
| MC-1 | REQUIRED | Model card MUST exist and be accessible |
| MC-2 | REQUIRED | Intended use cases MUST be explicitly documented |
| MC-3 | REQUIRED | Known limitations and failure modes MUST be listed |
| MC-4 | HIGH | Performance metrics across demographic groups (if applicable) |
| MC-5 | HIGH | Training data summary MUST be provided |
| MC-6 | MEDIUM | Bias and fairness evaluation results |
| MC-7 | MEDIUM | Environmental impact (CO2 emissions) |
| MC-8 | LOW | Model card version MUST match model version |

### Verification Steps

1. **Existence Check**: Verify model card is present at expected location (README.md, MODEL_CARD.md, or metadata endpoint)

2. **Schema Validation**: Validate against Hugging Face model card YAML schema:
   ```yaml
   Required fields:
   - model_description
   - intended_uses
   - limitations
   - training_data (reference)
   - evaluation_results
   ```

3. **Completeness Audit**: Check all REQUIRED and HIGH priority fields are populated with non-placeholder content

4. **Version Alignment**: Verify model card version matches model artifact version

5. **Freshness Check**: Ensure model card is not older than model artifact

### Block Conditions

- Gate BLOCKS if MC-1, MC-2, or MC-3 criteria are not met
- Gate BLOCKS if model card version does not match model version
- Gate WARNs if MC-4, MC-5 criteria are missing

### Output

```json
{
  "gate": "MODEL_CARD_VERIFICATION",
  "status": "PASS|BLOCK|WARN",
  "model_id": "<model_identifier>",
  "card_location": "<url_or_path>",
  "checks": {
    "exists": true|false,
    "schema_valid": true|false,
    "completeness_score": 0.0-1.0,
    "version_aligned": true|false
  },
  "missing_fields": ["list", "of", "missing", "fields"],
  "recommendations": ["actionable", "recommendations"]
}
```

---

## Gate 19: Training Data Provenance Gate

### Purpose

Verify the provenance, quality, and legal compliance of training data used by ML models. Ensure data lineage is traceable from source to model.

### Rationale

Data poisoning attacks (OWASP ML02:2023) occur when attackers manipulate training data. Without provenance tracking, malicious data injection cannot be detected. OpenLineage standards emphasize that "data lineage is the foundation for a new generation of powerful, context-aware data tools."

### Gate Criteria

| Criterion | Priority | Description |
|-----------|----------|-------------|
| DP-1 | REQUIRED | Training data sources MUST be documented |
| DP-2 | REQUIRED | Data license MUST be compatible with intended use |
| DP-3 | REQUIRED | Data collection methodology MUST be documented |
| DP-4 | HIGH | Data preprocessing steps MUST be documented |
| DP-5 | HIGH | Data freshness/age MUST be documented |
| DP-6 | HIGH | Data quality metrics MUST be provided |
| DP-7 | MEDIUM | Data lineage graph MUST be available |
| DP-8 | MEDIUM | PII/sensitive data handling documented |
| DP-9 | LOW | Data augmentation sources documented |

### Verification Steps

1. **Source Verification**: Verify all training data sources are:
   - From reputable/known sources, OR
   - Accompanied by collection methodology documentation

2. **License Compliance**: Verify data license permits:
   - Commercial use (if applicable)
   - Modification/derivation
   - Redistribution
   - Compatibility with output model license

3. **Quality Check**: Verify data quality metrics are present:
   - Label accuracy/validation method
   - Missing data percentage
   - Class distribution (for classification tasks)
   - Data cleaning procedures applied

4. **Lineage Validation**: Using OpenLineage-compatible metadata:
   - Verify dataset has unique identifier
   - Verify upstream dependencies are documented
   - Verify transformation lineage is traceable

5. **Taint Analysis**: Check if training data sources appear on:
   - Known poisoned dataset registries
   - Data ban lists
   - Copyright infringement notices

### Block Conditions

- Gate BLOCKS if DP-1 or DP-2 criteria are not met
- Gate BLOCKS if license is incompatible with intended use
- Gate BLOCKS if data source appears on security watchlists
- Gate WARNs if data quality metrics are missing

### Output

```json
{
  "gate": "TRAINING_DATA_PROVENANCE",
  "status": "PASS|BLOCK|WARN",
  "dataset_id": "<dataset_identifier>",
  "checks": {
    "sources_documented": true|false,
    "license_compliant": true|false,
    "quality_metrics_present": true|false,
    "lineage_verified": true|false,
    "taint_check_passed": true|false
  },
  "license": {
    "type": "<license_type>",
    "commercial_use_allowed": true|false,
    "attribution_required": true|false,
    "sharealike_required": true|false
  },
  "quality_score": 0.0-1.0,
  "warnings": ["list", "of", "warnings"]
}
```

---

## Gate 20: ML Supply Chain Integrity Gate

### Purpose

Verify the integrity and security of the ML supply chain including model dependencies, pre-trained weights, and MLOps infrastructure components.

### Rationale

OWASP ML06:2023 identifies ML Supply Chain Attacks as a critical risk. "This category is broad and important, as software supply chain in Machine Learning includes even more elements than in the case of classic software."

### Gate Criteria

| Criterion | Priority | Description |
|-----------|----------|-------------|
| SC-1 | REQUIRED | All model dependencies MUST be from trusted sources |
| SC-2 | REQUIRED | Model artifacts MUST have verifiable checksums/signatures |
| SC-3 | REQUIRED | Base models (if fine-tuned) MUST be verified |
| SC-4 | HIGH | Dependency vulnerabilities scanned within 7 days |
| SC-5 | HIGH | Model hub/repository access MUST be authenticated |
| SC-6 | MEDIUM | Container images for inference MUST be signed |
| SC-7 | MEDIUM | MLOps platform versions MUST be current |
| SC-8 | LOW | SBOM (Software Bill of Materials) available |

### Verification Steps

1. **Dependency Audit**: For all ML dependencies (PyTorch, TensorFlow, HuggingFace, etc.):
   - Verify package signatures
   - Check against known vulnerability databases (OSV, NVD)
   - Verify installation from official repositories

2. **Model Artifact Verification**:
   - Verify SHA256 checksums match published values
   - Verify GPG signatures if available
   - Check model has not been tampered with (file size, metadata)

3. **Base Model Chain**: For fine-tuned or derived models:
   - Recursively verify base model integrity
   - Validate derivation chain (base_model metadata)
   - Ensure all ancestors pass supply chain checks

4. **Infrastructure Scan**: For MLOps infrastructure:
   - Verify platform versions are not EOL
   - Check for known CVEs in MLOps tools
   - Verify secure configuration (no default passwords, TLS enabled)

5. **Repository Verification**: If using public model hubs:
   - Verify organization/author identity
   - Check model download count and community trust signals
   - Verify model repository has not been compromised

### Block Conditions

- Gate BLOCKS if SC-1, SC-2, or SC-3 criteria are not met
- Gate BLOCKS if any dependency has CRITICAL severity CVE
- Gate BLOCKS if model artifact checksum verification fails
- Gate WARNs if HIGH severity CVEs are present

### Output

```json
{
  "gate": "ML_SUPPLY_CHAIN_INTEGRITY",
  "status": "PASS|BLOCK|WARN",
  "model_id": "<model_identifier>",
  "checks": {
    "dependencies_verified": true|false,
    "checksum_valid": true|false,
    "base_model_verified": true|false,
    "vulnerability_scan_passed": true|false
  },
  "dependencies": [
    {
      "name": "<package_name>",
      "version": "<version>",
      "source": "<repository>",
      "signature_valid": true|false,
      "vulnerabilities": []
    }
  ],
  "vulnerabilities": {
    "critical": 0,
    "high": 0,
    "medium": 0,
    "low": 0
  },
  "sbom_available": true|false
}
```

---

## Gate 21: Adversarial Robustness Gate

### Purpose

Assess model resilience against adversarial attacks including input manipulation, evasion attacks, and prompt injection (for LLMs).

### Rationale

OWASP ML01:2023 (Input Manipulation Attack) and ML03:2023 (Model Inversion) represent significant threats. Models used by coding agents must maintain reliable behavior even when facing adversarial inputs.

### Gate Criteria

| Criterion | Priority | Description |
|-----------|----------|-------------|
| AR-1 | REQUIRED | Input validation schema MUST be defined |
| AR-2 | REQUIRED | Model MUST have defined confidence thresholds |
| AR-3 | HIGH | Adversarial testing performed (if applicable) |
| AR-4 | HIGH | Out-of-distribution detection capability |
| AR-5 | MEDIUM | Gradient masking or adversarial training applied |
| AR-6 | MEDIUM | Rate limiting on inference endpoints |
| AR-7 | MEDIUM | Input sanitization for prompt injection (LLMs) |
| AR-8 | LOW | Certified defenses documented |

### Verification Steps

1. **Input Schema Validation**: Verify:
   - Input format constraints are defined
   - Type checking is enforced
   - Range validation is implemented
   - Unexpected input handling is defined

2. **Adversarial Testing** (for applicable model types):
   - FGSM (Fast Gradient Sign Method) test results
   - PGD (Projected Gradient Descent) test results
   - Black-box attack resilience
   - Perturbation budget and impact metrics

3. **Prompt Injection Testing** (for LLMs):
   - Direct injection attempts
   - Indirect injection attempts
   - System prompt leakage tests
   - Jailbreak attempt results

4. **Confidence Calibration**: Verify:
   - Model outputs confidence scores
   - Confidence threshold for low-confidence rejection
   - Calibration metrics (ECE - Expected Calibration Error)

5. **Rate Limiting**: Verify inference endpoints have:
   - Request rate limiting
   - Abnormal pattern detection
   - IP-based throttling if appropriate

### Block Conditions

- Gate BLOCKS if AR-1 criterion is not met
- Gate BLOCKS if model fails basic adversarial tests without mitigation
- Gate BLOCKS if prompt injection vulnerabilities are exploitable in LLMs
- Gate WARNs if adversarial testing has not been performed

### Output

```json
{
  "gate": "ADVERSARIAL_ROBUSTNESS",
  "status": "PASS|BLOCK|WARN",
  "model_id": "<model_identifier>",
  "checks": {
    "input_validation_defined": true|false,
    "confidence_thresholds_set": true|false,
    "adversarial_testing_complete": true|false,
    "prompt_injection_tested": true|false
  },
  "adversarial_results": {
    "fgsm_accuracy": 0.0-1.0,
    "pgd_accuracy": 0.0-1.0,
    "black_box_resilience": "high|medium|low"
  },
  "prompt_injection": {
    "direct_injection_resistant": true|false,
    "indirect_injection_resistant": true|false,
    "tested_framework": "<framework_name>"
  },
  "calibration": {
    "ece_score": 0.0-1.0,
    "threshold": 0.0-1.0
  }
}
```

---

## Gate 22: Privacy-Preserving Output Gate

### Purpose

Ensure model outputs do not leak sensitive training data, model architecture details, or enable membership inference attacks.

### Rationale

OWASP ML03:2023 (Model Inversion) and ML04:2023 (Membership Inference) attacks can extract sensitive information from models. OWASP states: "Model inversion attacks occur when an attacker reverse-engineers the model to extract information from it."

### Gate Criteria

| Criterion | Priority | Description |
|-----------|----------|-------------|
| PP-1 | REQUIRED | Output logging MUST NOT include raw training data |
| PP-2 | REQUIRED | Confidence scores MUST be calibrated or masked |
| PP-3 | HIGH | Membership inference attack testing performed |
| PP-4 | HIGH | Model inversion resistance tested (if CV model) |
| PP-5 | MEDIUM | Differential privacy budget documented (if applied) |
| PP-6 | MEDIUM | Output caching has TTL limits |
| PP-7 | MEDIUM | PII detection on outputs |
| PP-8 | LOW | Model extraction resistance tested |

### Verification Steps

1. **Output Audit**: Review model outputs for:
   - Direct memorization of training examples
   - PII leakage in generated text
   - Reconstruction of sensitive features

2. **Membership Inference Testing**: Using shadow model or benchmark attacks:
   - Test if model reveals membership in training set
   - Measure attack precision/recall
   - Verify privacy leakage metrics

3. **Model Inversion Testing** (for applicable models):
   - Attempt to reconstruct training inputs from outputs
   - Measure reconstruction quality
   - Verify defense mechanisms (gradient clipping, regularization)

4. **Confidence Score Analysis**:
   - Verify confidence scores don't leak membership info
   - Check if low-confidence outputs are properly masked
   - Ensure calibration doesn't expose training set structure

5. **Output Logging Review**: Verify:
   - Training data not logged with outputs
   - Sensitive intermediate representations not exposed
   - Error messages don't leak model architecture details

6. **Differential Privacy Check** (if claimed):
   - Verify epsilon (privacy budget) is documented
   - Verify delta parameter is specified
   - Check privacy accounting methodology

### Block Conditions

- Gate BLOCKS if PP-1 or PP-2 criteria are not met
- Gate BLOCKS if membership inference attack achieves >70% precision
- Gate BLOCKS if PII is detected in model outputs
- Gate WARNs if privacy testing has not been performed

### Output

```json
{
  "gate": "PRIVACY_PRESERVING_OUTPUT",
  "status": "PASS|BLOCK|WARN",
  "model_id": "<model_identifier>",
  "checks": {
    "output_logging_safe": true|false,
    "confidence_masked": true|false,
    "membership_inference_tested": true|false,
    "pii_scan_passed": true|false
  },
  "membership_inference": {
    "attack_precision": 0.0-1.0,
    "attack_recall": 0.0-1.0,
    "privacy_leakage": "high|medium|low"
  },
  "differential_privacy": {
    "applied": true|false,
    "epsilon": "<budget>|null",
    "delta": "<parameter>|null"
  },
  "pii_detection": {
    "pii_found": true|false,
    "types_detected": ["email", "phone", "ssn", "other"]
  }
}
```

---

## Integration with 17-Gate Protocol

### Gate Sequence

The ML gates should be inserted into the existing protocol as follows:

```
Existing Gates 1-17: Standard Code Quality Gates
  ↓
Gate 18: MODEL_CARD_VERIFICATION (before ML model usage)
  ↓
Gate 19: TRAINING_DATA_PROVENANCE (before training/fine-tuning)
  ↓
Gate 20: ML_SUPPLY_CHAIN_INTEGRITY (before model download)
  ↓
Gate 21: ADVERSARIAL_ROBUSTNESS (before model deployment)
  ↓
Gate 22: PRIVACY_PRESERVING_OUTPUT (before output production)
```

### Conditional Execution

ML gates should execute conditionally based on context:

| Trigger Condition | Gates Executed |
|-------------------|----------------|
| Code imports ML library (torch, tensorflow, etc.) | 18, 20 |
| Code trains or fine-tunes a model | 18, 19, 20, 21 |
| Code loads a pre-trained model | 18, 20, 21 |
| Code deploys a model to production | 18, 20, 21, 22 |
| Code uses LLM APIs | 18, 20, 21, 22 |

### Overlap with Existing Gates

| Existing Gate | ML Gate Relationship |
|---------------|---------------------|
| Gate 1: Static Analysis | Extends to ML-specific linters (bandit, torch-analyzer) |
| Gate 4: Dependency Audit | Extends to ML package vulnerability scanning |
| Gate 7: Secret Scanning | Extends to API key detection for ML services |
| Gate 12: License Compliance | Extends to data and model licensing |
| Gate 17: Documentation | Extends to require Model Cards |

---

## Implementation Recommendations

### Tooling

| Gate | Recommended Tools |
|------|-------------------|
| Gate 18 | Hugging Face model card validator, custom schema validator |
| Gate 19 | OpenLineage, Pachyderm, MLflow Tracking |
| Gate 20 | OWASP Dependency-Check, Snyk, Sigstore/cosign |
| Gate 21 | Foolbox, ART (Adversarial Robustness Toolbox), Garak |
| Gate 22 | Membership Inference Privacy (MIA), TensorFlow Privacy |

### CI/CD Integration

```yaml
# Example GitHub Actions integration
ml-gates:
  runs-on: ubuntu-latest
  steps:
    - name: Gate 18 - Model Card Verification
      uses: ml-gates/model-card-validator@v1
      with:
        model-path: ./models/
        
    - name: Gate 19 - Training Data Provenance
      uses: ml-gates/data-provenance@v1
      with:
        lineage-endpoint: ${{ secrets.LINEAGE_API }}
        
    - name: Gate 20 - Supply Chain Integrity
      uses: ml-gates/supply-chain-check@v1
      with:
        sbom-output: sbom.json
        
    - name: Gate 21 - Adversarial Robustness
      uses: ml-gates/adversarial-test@v1
      with:
        test-suite: basic
        
    - name: Gate 22 - Privacy Preserving Output
      uses: ml-gates/privacy-check@v1
      with:
        pii-detection: true
```

### Configuration File

```yaml
# .ml-gates.yaml
version: "1.0"

model_card:
  required_fields:
    - model_description
    - intended_uses
    - limitations
    - training_data
  schema: huggingface

data_provenance:
  lineage_backend: openlineage
  required_licenses:
    - apache-2.0
    - mit
    - permissive
  banned_sources:
    - known-poisoned-registry

supply_chain:
  vulnerability_threshold:
    critical: 0
    high: 0
    medium: 10
  verify_signatures: true
  require_sbom: true

adversarial:
  test_framework: foolbox
  minimum_accuracy_under_attack: 0.5
  prompt_injection_tests:
    - direct
    - indirect
    - jailbreak

privacy:
  membership_inference_threshold: 0.6
  check_pii: true
  differential_privacy_minimum_epsilon: 10.0
```

---

## References

### Academic Papers

1. Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... & Gebru, T. (2019). Model cards for model reporting. In *Proceedings of the conference on fairness, accountability, and transparency* (pp. 220-229). [arXiv:1810.03993](https://arxiv.org/abs/1810.03993)

2. Bhure, S., & Singh, S. (2023). OWASP Machine Learning Security Top 10. *OWASP Foundation*. https://owasp.org/www-project-machine-learning-security-top-10/

### Standards and Specifications

3. Hugging Face. (2024). Model Cards documentation. https://huggingface.co/docs/hub/model-cards

4. Hugging Face. (2024). Annotated Model Card Guidebook. https://huggingface.co/docs/hub/model-card-annotated

5. OpenLineage. (2024). OpenLineage Documentation. https://openlineage.io/

### Threat Frameworks

6. MITRE. (2024). ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems). https://atlas.mitre.org/

7. OWASP. (2023). OWASP AI Security and Privacy Guide. https://owasp.org/www-project-ai-security-and-privacy-guide/

### Tools

8. Foolbox - Python toolbox to create adversarial examples. https://github.com/bethgelab/foolbox

9. Adversarial Robustness Toolbox (ART) - IBM. https://github.com/Trusted-AI/adversarial-robustness-toolbox

10. Garak - LLM vulnerability scanner. https://github.com/leondz/garak

11. TensorFlow Privacy - Library for training ML models with differential privacy. https://github.com/tensorflow/privacy

---

## Appendix A: Model Card Template

```markdown
---
# Model Card Metadata (YAML frontmatter)
language:
  - en
license: apache-2.0
library_name: transformers
tags:
  - code-generation
  - python
datasets:
  - my-org/training-data
model-index:
  - name: My Code Model
    results:
      - task:
          type: text-generation
        dataset:
          name: HumanEval
        metrics:
          - name: pass@1
            value: 0.45
---

# Model Name

## Model Description

- **Developed by:** [Organization/Team]
- **Model type:** Transformer-based code generation model
- **Language(s):** Python, JavaScript, TypeScript
- **License:** Apache 2.0
- **Base model:** codellama/CodeLlama-7b-hf

## Intended Use

This model is intended for generating code snippets and completing partial code.

### Out of Scope Use

- Not for production deployment without safety review
- Not for generating security-critical code without human review
- Not for proprietary code generation without license review

## Bias, Risks, and Limitations

### Bias

- Training data may contain biased coding patterns
- May reproduce stereotypes present in open-source code

### Risks

- May generate insecure code patterns
- May reproduce copyrighted code segments

### Limitations

- Limited context window (2048 tokens)
- May hallucinate APIs or functions

## Training Details

### Training Data

- Dataset: Custom curated code dataset
- Size: 10B tokens
- Preprocessing: Deduplication, license filtering

### Training Procedure

- Optimizer: AdamW
- Learning rate: 2e-5
- Epochs: 3
- Hardware: 8x A100 GPUs

## Evaluation

### Testing Data

- HumanEval
- MBPP
- Custom security benchmark

### Results

| Benchmark | Score |
|-----------|-------|
| HumanEval | 45% |
| MBPP | 52% |

## Environmental Impact

- Carbon emitted: 50 kg CO2eq
- Compute region: us-west-2

## Citation

```bibtex
@misc{mymodel2024,
  title={My Code Model},
  author={My Team},
  year={2024}
}
```

## Model Card Contact

For questions about this model card, contact: ml-team@example.com
```

---

## Appendix B: Data Lineage Example (OpenLineage Format)

```json
{
  "eventType": "COMPLETE",
  "eventTime": "2024-01-15T10:30:00Z",
  "run": {
    "runId": "d9e1f2c3-...",
    "facets": {
      "parent": {
        "run": {
          "runId": "parent-run-id"
        }
      }
    }
  },
  "job": {
    "namespace": "ml-pipeline",
    "name": "model-training",
    "facets": {
      "sourceCode": {
        "language": "python",
        "version": "3.9"
      }
    }
  },
  "inputs": [
    {
      "namespace": "s3://my-bucket",
      "name": "training-data/v1",
      "facets": {
        "schema": {
          "fields": [...]
        },
        "dataSource": {
          "uri": "s3://my-bucket/training-data/v1",
          "name": "training-data"
        }
      }
    }
  ],
  "outputs": [
    {
      "namespace": "mlflow://models",
      "name": "my-model/v1",
      "facets": {
        "model": {
          "framework": "pytorch",
          "version": "2.0.1"
        }
      }
    }
  ]
}
```

---

*Document Version: 1.0*
*Last Updated: 2026-02-05*
