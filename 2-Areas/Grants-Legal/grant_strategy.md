# Grant Funding Strategy: R_V Research & Infrastructure

## Executive Summary

Based on research of current funding landscapes for AI safety, interpretability, and consciousness research, here are the top 3 grant targets with proposal angles, budget scenarios, and competitive positioning.

---

## üéØ TOP 3 GRANT TARGETS

### 1. COEFFICIENT GIVING (formerly Open Philanthropy) ‚Äî Navigating Transformative AI Fund
**Priority: HIGHEST | Timeline: Rolling applications**

| Attribute | Details |
|-----------|---------|
| **Focus Areas** | Technical AI safety, AI governance, capacity building |
| **Grant Size** | $50K-$500K typical; up to $2M+ for major initiatives |
| **Deadline** | Rolling; RFPs published periodically |
| **Success Factors** | Clear theory of change, measurable impact, strong team |

**Current Opportunities:**
- ‚úÖ AI Governance RFP (OPEN) ‚Äî Policy development, strategic analysis, international governance
- ‚úÖ Capacity Building RFP (OPEN) ‚Äî Talent pipeline, professional networks, discourse
- ‚ö†Ô∏è Technical AI Safety ‚Äî Currently no open RFP, but always accepting expressions of interest

**Contact:** technicalaisafety@coefficientgiving.org | longtermfuture@effectivealtruismfunds.org

---

### 2. LONG-TERM FUTURE FUND (LTFF) ‚Äî Effective Altruism Funds
**Priority: HIGH | Timeline: Quarterly review cycles**

| Attribute | Details |
|-----------|---------|
| **Focus Areas** | Existential risk reduction, AI safety/alignment, field-building |
| **Grant Size** | $2.5K-$150K typical; larger for established projects |
| **Deadline** | Rolling; quarterly payout decisions |
| **Success Factors** | Longtermist worldview alignment, cost-effectiveness |

**Recent Awards (Benchmarks):**
- Logan Smith: $40K (6-month LM tools for alignment research)
- Robert Miles: $121K (AI safety content/media)
- Jeffrey Ladish: $98K (cybersecurity & alignment risk assessment)

**Contact:** longtermfuture@effectivealtruismfunds.org

---

### 3. NSF ‚Äî National AI Research Institutes (Strengthening AI Theme)
**Priority: STRATEGIC | Timeline: 2025-2026 cycle**

| Attribute | Details |
|-----------|---------|
| **Focus Areas** | Grounding, Instructibility, Alignment (3 required goals) |
| **Grant Size** | $16M-$20M over 4-5 years ($4M/year average) |
| **Deadline** | Next cycle likely late 2025/early 2026 |
| **Success Factors** | Multi-institution collaboration, workforce development |

**Key Requirements:**
- Must address ALL 3 goals: Grounding, Instructibility, Alignment
- Use-inspired research with clear application domain
- Multi-organizational collaboration required
- Strong education/workforce development component

**Contact:** AIInstitutesProgram@nsf.gov

---

## üí∞ BUDGET SCENARIOS

### Scenario A: Bootstrap ($50K-$150K)
**Target:** LTFF + Coefficient (capacity building)

| Item | Cost |
|------|------|
| Researcher stipends (2 FTE, 6 months) | $60K |
| Compute credits (GPUs, cloud) | $20K |
| Infrastructure & tooling | $15K |
| Publication & conference travel | $10K |
| Contingency (10%) | $10.5K |
| **TOTAL** | **~$115K** |

**Deliverables:**
- 2-3 published papers
- Open-source tooling release
- Workshop/conference presentations
- Field building (blog posts, community engagement)

---

### Scenario B: Scale-Up ($300K-$600K)
**Target:** Coefficient Technical AI Safety + LTFF

| Item | Cost |
|------|------|
| Core team (3-4 FTE, 12 months) | $250K |
| Compute infrastructure | $80K |
| Research assistants / interns | $60K |
| Equipment & software licenses | $40K |
| Travel & dissemination | $30K |
| Overhead (15%) | $69K |
| **TOTAL** | **~$530K** |

**Deliverables:**
- 6-8 published papers (top venues)
- Open-source framework/toolkit
- Workshop organization
- Collaboration with 2+ institutions
- Policy briefs / technical reports

---

### Scenario C: Institute ($2M-$5M)
**Target:** NSF AI Research Institutes + Coefficient major grants

| Item | Cost |
|------|------|
| Full research team (8-10 FTE) | $1.2M |
| Postdocs & graduate students | $600K |
| Compute cluster / infrastructure | $500K |
| Equipment & labs | $300K |
| Education & workforce programs | $200K |
| Admin & operations | $200K |
| **TOTAL (Year 1)** | **~$3M** |

**Deliverables:**
- Multi-year research program
- Graduate training pipeline
- Industry/government partnerships
- Transformative technology transfer

---

## üìù PROPOSAL ANGLES

### Angle 1: AI Safety through Interpretability & Mechanistic Understanding
**Best for:** Coefficient Technical AI Safety, NSF Strengthening AI

**Narrative:**
> "Current AI systems are increasingly capable but fundamentally opaque. We propose to develop mechanistic interpretability techniques that reveal how large language models represent concepts, enabling detection of dangerous behaviors before deployment. Our approach combines reverse engineering of neural networks with formal verification methods."

**Keywords:** mechanistic interpretability, feature circuits, superposition, activation patching, representational geometry

**Compelling Hook:**
- Directly addresses "Grounding" goal (NSF requirement)
- High tractability ‚Äî growing body of techniques
- Clear path to industry adoption

---

### Angle 2: Consciousness Research for AI Alignment
**Best for:** Coefficient (high-risk/high-reward), Philanthropic Funders

**Narrative:**
> "As AI systems approach human-level cognition, understanding whether and when they might have morally significant experiences becomes urgent. We propose a research program integrating computational neuroscience, philosophy of mind, and AI safety to develop empirical frameworks for detecting and reasoning about AI consciousness."

**Keywords:** artificial consciousness, moral patienthood, integrated information theory, global workspace theory, computational phenomenology

**Compelling Hook:**
- Addresses neglected, high-stakes question
- Interdisciplinary ‚Äî bridges technical and philosophical communities
- Positions team as thought leaders

**Risk:** More speculative; requires strong theoretical grounding

---

### Angle 3: Trustworthy AI through Hybrid Neuro-Symbolic Systems
**Best for:** NSF Strengthening AI, Coefficient Governance

**Narrative:**
> "Purely statistical AI systems lack the reliability guarantees needed for high-stakes applications. We propose to integrate neural networks with symbolic reasoning to create AI systems that are both capable and verifiably safe, with applications to autonomous systems and critical infrastructure."

**Keywords:** neuro-symbolic AI, formal verification, hybrid architectures, provable guarantees, trustworthy AI

**Compelling Hook:**
- Addresses "Instructibility" and "Alignment" goals
- Clear use cases (healthcare, autonomous vehicles)
- Aligns with NIST AI Risk Management Framework

---

## üìÖ GRANT CALENDAR 2025-2026

| Quarter | Action | Target |
|---------|--------|--------|
| **Q1 2025** | Submit LOI to Coefficient (AI Governance) | Coefficient |
| **Q1 2025** | Draft LTFF application | LTFF |
| **Q2 2025** | Submit Coefficient Technical AI Safety EOI | Coefficient |
| **Q2 2025** | Prepare NSF preliminary proposal materials | NSF |
| **Q3 2025** | LTFF grant decision expected | LTFF |
| **Q3 2025** | Submit Coefficient full proposal | Coefficient |
| **Q4 2025** | NSF preliminary proposal deadline (anticipated) | NSF |
| **Q1 2026** | NSF full proposal deadline (if invited) | NSF |
| **Q2 2026** | First awards notification | NSF |

**Immediate Actions (Next 30 Days):**
1. [ ] Draft expression of interest to technicalaisafety@coefficientgiving.org
2. [ ] Prepare LTFF application (can submit immediately)
3. [ ] Identify potential NSF collaborators for multi-institution proposal
4. [ ] Create 1-page project summaries for each angle

---

## üèÜ COMPETITIVE ADVANTAGES

### What Makes R_V Research Competitive

| Advantage | Why It Matters |
|-----------|----------------|
| **Positioned at intersection** | AI safety + consciousness + interpretability = unique angle |
| **Technical depth** | Hands-on experience with frontier LLMs |
| **Independent voice** | Not affiliated with big tech; can ask hard questions |
| **Agile & focused** | Faster iteration than large academic labs |
| **Cross-disciplinary** | Bridges technical and philosophical communities |

### Differentiation vs Other Applicants

**vs Academic Labs:**
- ‚úÖ Faster iteration, less bureaucracy
- ‚úÖ Direct focus on safety (not just capability)
- ‚ùå Less institutional infrastructure
- ‚ùå Smaller publication history

**vs Industry Safety Teams:**
- ‚úÖ Independent from commercial pressures
- ‚úÖ Can publish openly
- ‚úÖ Focus on fundamental research
- ‚ùå Less compute access
- ‚ùå No product integration

**vs Other Indie Researchers:**
- ‚úÖ More structured/institutional approach
- ‚úÖ Clear research agenda
- ‚úÖ Team capabilities
- ‚ö†Ô∏è Need to demonstrate track record

---

## üîë KEYS TO SUCCESS

### For Coefficient / LTFF Applications:
1. **Clear theory of change** ‚Äî How does this research reduce existential risk?
2. **Cost-effectiveness** ‚Äî Why is this the best use of marginal dollars?
3. **Track record** ‚Äî Demonstrate ability to execute (publications, code, etc.)
4. **Collaboration potential** ‚Äî Show connections to broader ecosystem

### For NSF Applications:
1. **Multi-institution letters of intent** ‚Äî Start networking NOW
2. **Workforce development plan** ‚Äî Required component
3. **Use-inspired context** ‚Äî Connect to real application domain
4. **All 3 goals addressed** ‚Äî Grounding, Instructibility, Alignment

### General Best Practices:
- **Start small, build credibility** ‚Äî LTFF ‚Üí Coefficient ‚Üí NSF pipeline
- **Publish aggressively** ‚Äî Preprints + top venues (NeurIPS, ICML, ICLR)
- **Engage community** ‚Äî Workshops, blog posts, Twitter/X presence
- **Open source everything** ‚Äî Code, data, models

---

## üìä RECOMMENDED STRATEGY

### Phase 1: Bootstrap (Months 1-6)
- **Target:** LTFF ($50-150K)
- **Goal:** Fund initial research, generate publications
- **Angle:** AI Safety through Interpretability

### Phase 2: Scale (Months 6-18)
- **Target:** Coefficient Technical AI Safety ($300-600K)
- **Goal:** Expand team, build infrastructure
- **Angle:** Mechanistic Interpretability + Consciousness Research

### Phase 3: Institute (Months 18-36)
- **Target:** NSF AI Research Institutes ($2M+/year)
- **Goal:** Establish long-term research program
- **Angle:** Strengthening AI ‚Äî All 3 Goals

---

## üìû CONTACTS & RESOURCES

| Funder | Primary Contact | Secondary |
|--------|-----------------|-----------|
| Coefficient (Tech Safety) | technicalaisafety@coefficientgiving.org | Claire Zabel (Managing Director) |
| Coefficient (Governance) | See RFP page | Luke Muehlhauser |
| LTFF | longtermfuture@effectivealtruismfunds.org | Caleb Parikh (Project Lead) |
| NSF AI Institutes | AIInstitutesProgram@nsf.gov | 703-292-5111 |

---

*Document prepared: 2026-02-05*
*Next review: Quarterly or when new RFPs released*
