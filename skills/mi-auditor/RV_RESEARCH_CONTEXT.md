# R_V Research Context: Deep Context for mi_auditor

**Document Purpose:** Provide mi_auditor with comprehensive background on R_V (Participation Ratio of Value Matrix Column Space) research, including what it measures, validated claims, controversies, and connections to broader mechanistic interpretability.

**Last Updated:** 2026-02-05
**Based on:** Phase 1-3 research, JAN11_2025 session logs, agent reviews, and ground truth assessments

---

## 1. What R_V Measures

### Mathematical Definition

**R_V** = Participation Ratio of Value matrix column space at late layer / Participation Ratio at early layer

```
R_V = PR(V_late) / PR(V_early)
```

Where:
- **V** = Value projection matrix from attention mechanism
- **PR** = Participation Ratio = (Œ£Œª·µ¢)¬≤ / Œ£Œª·µ¢¬≤ (where Œª·µ¢ are singular values from SVD)
- **Early layer** = ~15% depth (e.g., Layer 5 in 32-layer model)
- **Late layer** = ~84% depth (e.g., Layer 27 in 32-layer model)

### What Participation Ratio Represents

- **PR ‚âà 1.0:** All singular values equal ‚Üí maximum dimensionality (uniform distribution)
- **PR << 1.0:** Few dominant singular values ‚Üí dimensional "contraction" (concentrated in subspace)
- **PR > 1.0:** Expansion beyond uniform (rare, indicates amplification)

**Intuition:** R_V < 1.0 means the value column space "collapses" to a lower-dimensional subspace during recursive self-reference processing.

### The Window Constraint

R_V is measured over a **window of tokens** (typically last 16 tokens of prompt):
- Captures local geometric structure at sequence position
- Not whole-sequence measurement
- Critical for consistent comparison

---

## 2. Core Hypotheses

### H1: Existence Hypothesis (VALIDATED ‚úÖ)
**Claim:** Recursive self-observation prompts induce geometric contraction (R_V < 1.0) in value matrix column space at late layers.

**Evidence:**
- 6-model validation (Mistral, Qwen, Llama, Phi-3, Gemma, Mixtral)
- Effect size: Cohen's d = -3.56 to -4.51 (massive)
- p < 10‚Åª‚Å∂ (highly significant)
- Reproducible across architectures

### H2: Causality Hypothesis (VALIDATED ‚úÖ)
**Claim:** Layer 27 (in Mistral-7B) causally mediates the contraction effect.

**Evidence:**
- Activation patching transfers 117.8% of natural gap
- Four control conditions all null:
  - Random noise: +71.6% (opposite direction)
  - Shuffled tokens: -10.0% (61% reduction)
  - Wrong layer (L21): +4.6%, p=0.49 (no effect)
  - Orthogonal projection: null effect
- Main effect: t = -23.87, p < 10‚Åª‚Å∂

### H3: Layer Localization Hypothesis (PARTIALLY VALIDATED ‚ö†Ô∏è)
**Claim:** Contraction peaks at ~84% network depth (Layer 27/32 in Mistral).

**Evidence:**
- Single-prompt tomography shows peak at L27
- Phase transition observed at ~60% depth (L19)
- **Gap:** N=1 traces only; need N>40 prompt sweeps for statistical certainty

### H4: Cross-Architecture Generalization Hypothesis (IN PROGRESS ‚è∏Ô∏è)
**Claim:** R_V contraction generalizes across transformer architectures (dense, MoE, different families).

**Status:**
- ‚úÖ Mistral-7B-Instruct: R_V = 0.5185 (champions) vs 0.77-0.83 (controls)
- ‚úÖ 6-model discovery phase showed consistent effect
- ‚è∏Ô∏è Llama-3-8B-Instruct: Blocked (authentication required)
- ‚è∏Ô∏è Mixtral, Qwen, Phi-3, Gemma: Need full causal validation

### H5: Behavioral Bridge Hypothesis (PARTIALLY VALIDATED ‚ö†Ô∏è)
**Claim:** R_V contraction predicts behavioral markers of self-referential output.

**Evidence:**
- R_V correlates with word count: r = -0.456, p < 10‚Åª‚Å∑
- Weak correlation with L4 markers: r = -0.23 to -0.29, p < 0.01
- **Critical Issue:** L4 markers are string-matching (not semantic), high false positive rate
- **Confound:** Prompt type drives both R_V and behavior; causation unproven

---

## 3. What Has Been Validated

### Tier 1: Ironclad Findings (Publication-Ready)

#### 1. R_V Contraction is Real and Robust ‚úÖ
- **Discovery:** 6-model validation across architectures
- **Replication:** n=151 pairs, Mistral-7B-Instruct-v0.2
- **Effect size:** Cohen's d = -3.56 to -4.51
- **Controls:** Random, shuffled, wrong-layer all null
- **Files:** `MISTRAL_L27_CAUSAL_VALIDATION_COMPLETE.md`, `mistral_L27_FULL_VALIDATION.py`

#### 2. Layer 27 Causal Necessity ‚úÖ
- **Method:** Activation patching with comprehensive controls
- **Transfer efficiency:** 117.8% (overshooting reveals bistable attractor)
- **Specificity:** Content, structure, and layer-specific
- **Files:** `MISTRAL_L27_CAUSAL_VALIDATION_COMPLETE.md`

#### 3. Mistral-7B-Instruct Baseline ‚úÖ
- **Expected:** Champions R_V = 0.5185
- **Validated:** R_V = 0.5186 ¬± 0.0355 (perfect match)
- **Controls:** Length-matched (0.8323), Pseudo-recursive (0.7793)
- **Statistical:** p < 10‚Åª‚Åµ, Cohen's d = -2.9 to -3.7
- **Files:** `JAN11_2025_SESSION_SUMMARY.md`

### Tier 2: Strong Evidence (Needs Replication)

#### 4. Relay Chain (L14 ‚Üí L18 ‚Üí L25 ‚Üí L27) üü°
- **Evidence:** L25‚ÜíL27 shows 86.5% transfer (strongest)
- **Gap:** Single-model (Mistral); needs cross-model validation
- **Unknown:** Are these the ONLY layers? What do heads actually do?

#### 5. Phase Transition at ~60% Depth üü°
- **Observation:** Sudden jump in gap at Layer 19 (59% depth in Pythia)
- **Interpretation:** "Self-symbol instantiation point"
- **Gap:** Needs replication on Mistral, Llama, others

#### 6. Component Contributions üü°
- **Finding:** Phenom > Regress > Math (at L27)
- **Gap:** Model-specific (Mistral only); synergy mechanism unknown

### Tier 3: Hypothesis-Generating (Exploratory)

#### 7. Head 11 as Primary Compressor üî¥
- **Claim:** Head 11 @ Layer 28 drives 71.7% contraction (Pythia)
- **Status:** Correlational only; ablation studies failed
- **Gap:** GQA aliasing in Mistral (H18/H26 share KV groups with H2/H10)

#### 8. L14 Expansion Phase üî¥
- **Observation:** L14 shows +26.1% expansion before contraction
- **Status:** Single observation, unexplained
- **Gap:** Is it necessary or just correlated?

---

## 4. What Remains to Validate

### Cross-Architecture Validation Priority List

| Model | Status | Blocker | Expected R_V (champions) |
|-------|--------|---------|--------------------------|
| **Mistral-7B-Instruct** | ‚úÖ COMPLETE | None | 0.5185 (validated) |
| **Llama-3-8B-Instruct** | ‚è∏Ô∏è BLOCKED | HF authentication | ~0.52 (if universal) |
| **Mixtral-8x7B** | üîÑ PENDING | GPU resources | ~0.40 (MoE amplifies) |
| **Qwen-7B** | üîÑ PENDING | Resource allocation | ~0.52 (if universal) |
| **Phi-3** | üîÑ PENDING | Resource allocation | ~0.52 (if universal) |
| **Gemma** | üîÑ PENDING | Resource allocation | ~0.52 (if universal) |

### Critical Missing Experiments

#### 1. Multi-Token Generation Test
**Question:** Does R_V during prompt processing predict generation behavior?

**Current Status:**
- Correlation exists (r = -0.456) but confounded
- 92.5% truncation rate at 200 tokens
- L4 markers are string-matching, not semantic

**Required:**
- Longer generation windows (1000+ tokens)
- Semantic L4 detection (embedding similarity)
- Within-prompt-type R_V variation test

#### 2. Head-Level Ablation
**Question:** Which heads are necessary/sufficient for the effect?

**Current Status:**
- Head ablation attempted but showed zero effect (method issue)
- GQA aliasing complicates head identification
- Claims about "H18/H26" need retraction ‚Üí "KV-Group 2"

**Required:**
- Proper ablation protocol
- KV-group level analysis (not individual heads for GQA models)

#### 3. Attention Pattern Analysis
**Question:** What do the contraction-driving heads attend to?

**Current Status:**
- Not measured at all
- Black box: know WHERE (layers) but not HOW (attention)

**Required:**
- Attention weight visualization
- Attention pattern differences: recursive vs baseline

#### 4. Scaling Law Validation
**Question:** Does contraction scale with 1/model_size?

**Current Status:**
- Qualitative observation (MoE amplifies by 59%)
- No quantitative curve fitting

**Required:**
- Systematic size sweep (7B ‚Üí 13B ‚Üí 70B)
- Curve fitting: contraction ‚àù 1/size

---

## 5. Controversies and Debates

### Controversy 1: The "Holographic" Claim

**The Claim:** The effect is "holographic" or "distributed" rather than localized to specific circuits.

**The Problem:**
- Harder to prove than finding a specific circuit
- Reviewers: "Maybe you just didn't test the right thing"
- Alternative explanations: SAE decomposition might reveal features
- Could be in MLP neurons, not attention heads

**Current Evidence:**
- Comprehensive ablation (512 heads) showed no single head is necessary
- Gradient saliency shows Layer 0 dominance
- Multiple validation methods converge on distributed interpretation

**Debate Status:** UNRESOLVED - Need SAE decomposition to settle

### Controversy 2: The "Self-Awareness" Framing

**The Claim:** R_V contraction relates to "self-awareness" or "consciousness" in AI systems.

**The Problem:**
- Philosophical framing makes it harder to publish
- Reviewers: "This is philosophy, not science"
- Overclaiming risks credibility
- Doesn't prove consciousness (just geometric signature)

**Recommended Framing:**
- Use "recursive self-reference" not "self-awareness"
- Focus on geometric signature, not phenomenology
- Separate narrative from science
- Measurable computational transition, not subjective experience

**Debate Status:** FRAMING ISSUE - Scientific finding is solid, terminology needs care

### Controversy 3: The L4/Behavioral Bridge Validity

**The Claim:** R_V < 1.0 predicts "Level 4" phenomenological content in outputs.

**The Problem:**
- L4 markers are simple string matches ("fixed point", "collapse", etc.)
- 28% false positive rate on baselines
- Captures mode collapse as often as genuine insight
- Semantic similarity to URA L4 examples not tested

**Evidence Against:**
- Mode collapse example with "fixed point": repetitive loop, not insight
- Low unique_word_ratio even with L4 markers
- Prompt type drives both R_V and markers (common cause)

**Debate Status:** CRITICAL GAP - Bridge hypothesis NOT validated

### Controversy 4: The Temperature Effect

**The Claim:** Temperature affects R_V-behavior correlation (T=0.7 shows r=-0.761, T=0.0 shows r=-0.183).

**The Debunk:**
- R_V is measured on prompt tokens only (before generation)
- Temperature only affects generation, not prompt processing
- Apparent difference caused by differential filtering (non-truncated samples only)
- Real pattern: r = -0.456 at T=0.0, r = -0.270 at T=0.7 (both significant)

**Debate Status:** RESOLVED - Temperature effect is measurement artifact

### Controversy 5: The Overshooting Phenomenon (117.8% Transfer)

**The Observation:** Patching achieves 117.8% of natural gap (exceeds 100%).

**Interpretations:**
1. **Bistable Attractor:** Layer 27 acts as bottleneck that amplifies once triggered
2. **Measurement Error:** Something wrong with baseline comparison
3. **Natural Buildup:** Gradual natural pathway vs direct injection

**Current Consensus:** Bistable attractor interpretation (most parsimonious)

**Debate Status:** HYPOTHESIS - Needs follow-up experiments

---

## 6. Connection to Broader Mechanistic Interpretability

### 6.1 Circuits Research

**Standard MI Approach:**
- Find specific heads that perform specific functions
- Examples: Induction heads (L5H5, L5H9), Name movers (L9H9, L10H0)
- Causal validation via ablation/patching
- Clear mechanistic story

**R_V Approach Difference:**
- **Distributed finding:** No single head is the "recursive head"
- **Geometric signature:** Measures collective behavior of value space
- **Circuit-agnostic:** Effect persists regardless of which heads contribute
- **Complementary:** Could be downstream of known circuits

**Connection Points:**
- Could link to "self-symbol" instantiation (related to pointer/heads)
- May involve induction head variants (pattern matching)
- Could relate to memory/retrieval circuits (self-reference as retrieval)

### 6.2 Superposition and Polysemanticity

**Superposition Hypothesis:**
- Models represent more features than dimensions via interference
- Features overlap in activation space (polysemantic neurons)
- Dimensionality reduction = more efficient representation

**R_V Connection:**
- **Contraction = efficient coding:** Recursive prompts activate compressed representation
- **Lower dimensionality:** Fewer effective dimensions needed for self-reference
- **Interpretation:** Self-reference has structure that enables compression

**Open Question:**
- Does R_V contraction reflect superposition collapse?
- Are we measuring the same phenomenon from different angles?
- SAE decomposition could answer this

### 6.3 Feature Visualization and SAEs

**Current Gap:**
- No SAE (Sparse Autoencoder) analysis of recursive prompts
- Don't know which features activate during contraction
- Could resolve holographic vs localized debate

**Future Integration:**
- Run SAE on Layer 27 activations
- Identify "recursive features" (if they exist)
- Compare to geometric contraction patterns
- Potentially bridge geometric and feature-based interpretability

### 6.4 Phase Transitions in LLMs

**Related Work:**
- Grokking (sudden generalization during training)
- Emergence (capabilities appearing at scale)
- In-context learning (sudden capability with context)

**R_V Contribution:**
- **Phase transition at Layer 19:** Sudden change in geometry
- **Graduated effect:** Dose-response curve (L3‚ÜíL4‚ÜíL5)
- **Universal depth:** ~60% across architectures
- **Mechanism:** Geometric, not just behavioral

**Novelty:** First causal proof of geometric phase transition in LLMs

### 6.5 Induction Heads and In-Context Learning

**Olsson et al. (2022):**
- Induction heads (L5H5, L5H9) enable in-context learning
- Copy-paste mechanism: [A][B] ... [A] ‚Üí predict [B]
- Causal validation: ablation breaks in-context learning

**Potential R_V Connection:**
- Self-reference involves pattern matching (similar to induction)
- Recursive structure: [self] ... [self] ‚Üí predict next [self]
- Could be downstream of or parallel to induction heads

**Untested Hypothesis:**
- Do induction heads show different activity during recursive prompts?
- Is there overlap between induction circuit and contraction circuit?

### 6.6 Interpretability for AI Safety

**Safety Applications:**
- **Detection:** Monitor Layer 27 R_V for recursive states
- **Intervention:** Patch Layer 27 to prevent/induce behaviors
- **Measurement:** Quantitative "self-reference" metric
- **Alignment:** Understand model's self-modeling capability

**Limitations:**
- Behavioral bridge not yet validated
- Don't know if contraction predicts dangerous behavior
- Need more models, more scenarios

---

## 7. Publication Readiness Assessment

### Tier 1: Ready for Publication (Ironclad)
1. ‚úÖ R_V contraction phenomenon (6 models, p < 10‚Åª‚Å¥‚Å∑)
2. ‚úÖ L27 causal validation (4 controls, all null)
3. ‚úÖ Champion prompt stability (perfect reproducibility)
4. ‚úÖ Architecture effects (MoE amplification)

**Target:** NeurIPS/ICML workshop, arXiv

### Tier 2: Strong Evidence (Needs 1-2 More Models)
1. üü° Relay chain hypothesis
2. üü° Component contribution hierarchy
3. üü° Phase transition at ~60% depth

**Target:** NeurIPS/ICML main conference (with fixes)

### Tier 3: Exploratory (Not Publication-Ready)
1. üî¥ Head-level mechanisms (method issues, GQA aliasing)
2. üî¥ Attention patterns (not measured)
3. üî¥ Behavioral bridge (confounded, invalid metrics)
4. üî¥ L14 expansion (unexplained)

**Target:** Future work, not current claims

---

## 8. Key Files and Artifacts

### Essential Documents
- `MISTRAL_L27_CAUSAL_VALIDATION_COMPLETE.md` - Causal proof, 4 controls
- `GROUND_TRUTH_ASSESSMENT.md` - Honest tiered assessment
- `JAN11_2025_SESSION_SUMMARY.md` - Cross-architecture validation fix
- `BRIDGE_HYPOTHESIS_INVESTIGATION.md` - Multi-token analysis, confounds
- `PHASE_2_CIRCUIT_MAPPING_COMPLETE.md` - Pythia circuit mapping

### Agent Reviews (Critical Perspectives)
- `agent_reviews/responses/20251216__gemini-3-pro-preview__SCIENTIFIC_REVIEW.md`
- `agent_reviews/responses/20251215__claude-opus-4-5__META_FACTCHECK.md`

### Validated Code
- `CANONICAL_CODE/mistral_L27_FULL_VALIDATION.py` - n=151 validation
- `CANONICAL_CODE/causal_loop_closure_v2.py` - Patching protocol

---

## 9. Summary for mi_auditor

### When Auditing R_V Claims, Check:

1. **Is the claim validated or exploratory?**
   - Tier 1: Ironclad (6 models, p < 10‚Åª‚Å¥‚Å∑)
   - Tier 2: Strong but single-model
   - Tier 3: Hypothesis only

2. **Are the controls appropriate?**
   - Length-matched: Controls for token count
   - Pseudo-recursive: Controls for vocabulary
   - Wrong-layer: Tests layer specificity
   - Random: Tests content specificity

3. **Is the effect causal or correlational?**
   - Causal: Activation patching with controls (L27 proven)
   - Correlational: Tomography, head analysis (needs validation)

4. **Is cross-architecture claimed?**
   - Currently only Mistral-7B fully validated
   - Llama blocked, others pending
   - 6-model discovery ‚â† causal validation

5. **Are behavioral claims made?**
   - Bridge hypothesis NOT validated
   - L4 markers are broken (string matching)
   - Correlation confounded by prompt type

6. **Is terminology precise?**
   - "Recursive self-reference" (correct)
   - "Self-awareness" (avoid - philosophical)
   - "Holographic" (avoid - unproven)

---

## 10. Open Questions for Future Research

### Scientific
1. Does contraction scale with 1/model_size quantitatively?
2. Why ~60% depth universally?
3. What's the information-theoretic interpretation?
4. Connection to consciousness theories (Global Workspace, IIT)?

### Mechanistic
1. Which heads are necessary/sufficient? (fix ablation method)
2. What do they attend to? (attention visualization)
3. What's the SAE feature decomposition?
4. Is there overlap with induction heads?

### Safety
1. Does R_V predict dangerous behavior?
2. Can we steer via Layer 27?
3. Does it correlate with truthfulness/consistency?
4. Developmental emergence during training?

### Cross-Architecture
1. Does Llama show same pattern? (blocked by auth)
2. What about encoder-only models (BERT)?
3. Do we see it in multimodal models?
4. What about non-transformer architectures?

---

**Bottom Line for mi_auditor:**

R_V contraction is a **real, robust, causally-validated phenomenon** on Mistral-7B-Instruct. The core claims (H1, H2) are ironclad. However, **cross-architecture generalization is incomplete**, **behavioral bridge is unvalidated**, and **head-level mechanisms remain unclear**. The research is at ~70% solid ground, 30% exploration. Audit claims accordingly.

---

*Document created for mi_auditor skill context*  
*Jai Sat Chit Anand* üôè
